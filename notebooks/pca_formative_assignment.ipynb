{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44fa5f41",
   "metadata": {},
   "source": [
    "# Formative 2 – Principal Component Analysis\n",
    "This notebook implements Principal Component Analysis (PCA) from scratch on an African development indicators dataset to satisfy the formative assignment requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989030b4",
   "metadata": {},
   "source": [
    "## Dataset overview\n",
    "We use a curated African Development Indicators dataset that includes socio-economic metrics for several countries between 2019 and 2020. The dataset intentionally contains missing values and categorical data (country names) to demonstrate cleaning, encoding, and PCA readiness steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce48991",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from time import perf_counter\n",
    "\n",
    "plt.style.use(\"seaborn-v0_8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8745190",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and inspect the dataset\n",
    "DATA_PATH = Path('..') / 'data' / 'african_development_indicators.csv'\n",
    "df_raw = pd.read_csv(DATA_PATH)\n",
    "print(f'Rows: {len(df_raw)}')\n",
    "display(df_raw.head())\n",
    "print('Data types and non-null counts:')\n",
    "display(df_raw.dtypes)\n",
    "print('Missing values per column:')\n",
    "display(df_raw.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d161663",
   "metadata": {},
   "source": [
    "## Data preparation\n",
    "We handle missing values by imputing numeric columns with their mean and categorical columns with the mode. Country names are one-hot encoded to retain regional information while producing a fully numeric matrix suitable for PCA. Finally, we standardize the features to zero mean and unit variance so that PCA is not dominated by scale differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ce6c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute missing values and encode categorical data\n",
    "df_clean = df_raw.copy()\n",
    "categorical_cols = df_clean.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "numeric_cols = df_clean.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "for col in numeric_cols:\n",
    "    df_clean[col] = df_clean[col].fillna(df_clean[col].mean())\n",
    "\n",
    "for col in categorical_cols:\n",
    "    df_clean[col] = df_clean[col].fillna(df_clean[col].mode().iat[0])\n",
    "\n",
    "df_encoded = pd.get_dummies(df_clean, columns=categorical_cols, drop_first=True)\n",
    "feature_names = df_encoded.columns\n",
    "\n",
    "X = df_encoded.to_numpy(dtype=float)\n",
    "X_mean = X.mean(axis=0)\n",
    "X_std = X.std(axis=0, ddof=0)\n",
    "X_std[X_std == 0] = 1.0  # prevent division by zero for constant features\n",
    "X_scaled = (X - X_mean) / X_std\n",
    "\n",
    "print('Prepared feature matrix shape:', X_scaled.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb0ea2b",
   "metadata": {},
   "source": [
    "## Task 1 – Implement PCA from first principles\n",
    "We compute the covariance matrix of the standardized data, perform eigen-decomposition, sort eigenvalues in descending order, and project the data onto the principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9564b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_covariance_matrix(X: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Return the sample covariance matrix for a centered dataset.\"\"\"\n",
    "    n_samples = X.shape[0]\n",
    "    X_centered = X - X.mean(axis=0)\n",
    "    covariance_matrix = (X_centered.T @ X_centered) / (n_samples - 1)\n",
    "    return covariance_matrix\n",
    "\n",
    "\n",
    "def eigen_decomposition(covariance_matrix: np.ndarray) -> tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Compute eigenvalues and eigenvectors sorted from largest to smallest eigenvalue.\"\"\"\n",
    "    eigenvalues, eigenvectors = np.linalg.eigh(covariance_matrix)\n",
    "    sort_idx = np.argsort(eigenvalues)[::-1]\n",
    "    eigenvalues_sorted = eigenvalues[sort_idx]\n",
    "    eigenvectors_sorted = eigenvectors[:, sort_idx]\n",
    "    return eigenvalues_sorted, eigenvectors_sorted\n",
    "\n",
    "\n",
    "def project_data(X: np.ndarray, eigenvectors: np.ndarray, n_components: int) -> np.ndarray:\n",
    "    \"\"\"Project data onto the first n_components principal directions.\"\"\"\n",
    "    return X @ eigenvectors[:, :n_components]\n",
    "\n",
    "\n",
    "def compute_explained_variance(eigenvalues: np.ndarray) -> tuple[np.ndarray, np.ndarray]:\n",
    "    total_variance = eigenvalues.sum()\n",
    "    explained_variance_ratio = eigenvalues / total_variance\n",
    "    cumulative_explained_variance = np.cumsum(explained_variance_ratio)\n",
    "    return explained_variance_ratio, cumulative_explained_variance\n",
    "\n",
    "\n",
    "def pca_from_scratch(X: np.ndarray, n_components: int | None = None) -> dict[str, np.ndarray]:\n",
    "    \"\"\"Perform PCA using covariance eigen-decomposition.\"\"\"\n",
    "    if n_components is None:\n",
    "        n_components = X.shape[1]\n",
    "\n",
    "    covariance_matrix = compute_covariance_matrix(X)\n",
    "    eigenvalues, eigenvectors = eigen_decomposition(covariance_matrix)\n",
    "    explained_variance_ratio, cumulative_variance = compute_explained_variance(eigenvalues)\n",
    "    projected_data = project_data(X, eigenvectors, n_components)\n",
    "\n",
    "    return {\n",
    "        'covariance_matrix': covariance_matrix,\n",
    "        'eigenvalues': eigenvalues,\n",
    "        'eigenvectors': eigenvectors,\n",
    "        'projected_data': projected_data,\n",
    "        'explained_variance_ratio': explained_variance_ratio,\n",
    "        'cumulative_explained_variance': cumulative_variance,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2765dce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run PCA on the standardized data\n",
    "pca_result = pca_from_scratch(X_scaled)\n",
    "explained_variance_df = pd.DataFrame({\n",
    "    'Eigenvalue': pca_result['eigenvalues'],\n",
    "    'Explained Variance Ratio': pca_result['explained_variance_ratio'],\n",
    "    'Cumulative Explained Variance': pca_result['cumulative_explained_variance'],\n",
    "})\n",
    "explained_variance_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93c6bc5",
   "metadata": {},
   "source": [
    "## Task 2 – Select principal components dynamically\n",
    "We choose the smallest number of components that achieve a target explained variance threshold. The helper below returns both the selected dimensionality and the transformed dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2f448e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_number_of_components(cumulative_variance: np.ndarray, threshold: float = 0.95) -> int:\n",
    "    \"\"\"Return the minimal number of components needed to reach the desired explained variance.\"\"\"\n",
    "    if threshold <= 0 or threshold > 1:\n",
    "        raise ValueError('threshold must be within (0, 1].')\n",
    "    n_components = int(np.searchsorted(cumulative_variance, threshold) + 1)\n",
    "    return n_components\n",
    "\n",
    "\n",
    "variance_threshold = 0.95\n",
    "n_components_optimal = select_number_of_components(\n",
    "    pca_result['cumulative_explained_variance'],\n",
    "    threshold=variance_threshold,\n",
    " )\n",
    "projected_optimal = pca_result['projected_data'][:, :n_components_optimal]\n",
    "print(f'Minimum components for {variance_threshold:.0%} variance: {n_components_optimal}')\n",
    "projected_optimal[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608d7154",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize explained variance profile\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "indices = np.arange(1, len(pca_result['explained_variance_ratio']) + 1)\n",
    "ax.bar(indices, pca_result['explained_variance_ratio'], alpha=0.7, label='Explained variance ratio')\n",
    "ax.plot(indices, pca_result['cumulative_explained_variance'], marker='o', color='black', label='Cumulative explained variance')\n",
    "ax.axhline(y=variance_threshold, color='red', linestyle='--', label=f'{variance_threshold:.0%} threshold')\n",
    "ax.set_xlabel('Principal component index')\n",
    "ax.set_ylabel('Variance ratio')\n",
    "ax.set_title('Explained variance by principal component')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b643e0",
   "metadata": {},
   "source": [
    "## Task 3 – Optimise for larger datasets\n",
    "The PCA implementation leverages vectorised NumPy operations. To demonstrate scalability, we benchmark the runtime on synthetic datasets with thousands of samples and features. This validates that the approach can process larger inputs efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d0b83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_pca_runtime(n_samples: int = 5000, n_features: int = 40, n_components: int = 10, runs: int = 3) -> dict[str, float]:\n",
    "    rng = np.random.default_rng(seed=42)\n",
    "    results = []\n",
    "    for _ in range(runs):\n",
    "        X_synthetic = rng.normal(size=(n_samples, n_features))\n",
    "        start = perf_counter()\n",
    "        pca_from_scratch(X_synthetic, n_components=n_components)\n",
    "        results.append(perf_counter() - start)\n",
    "    results = np.array(results)\n",
    "    return {\n",
    "        'mean_seconds': results.mean(),\n",
    "        'std_seconds': results.std(ddof=1),\n",
    "        'runs': runs,\n",
    "        'samples': n_samples,\n",
    "        'features': n_features,\n",
    "        'components': n_components,\n",
    "    }\n",
    "\n",
    "\n",
    "benchmark_stats = benchmark_pca_runtime()\n",
    "benchmark_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb1c76f",
   "metadata": {},
   "source": [
    "## Visualisations – Before and After PCA\n",
    "We compare the structure of the data in the original feature space against the first two principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a9192c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original feature space (two informative dimensions)\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "sns.scatterplot(\n",
    "    data=df_clean,\n",
    "    x='Access_to_electricity_pct',\n",
    "    y='Internet_users_pct',\n",
    "    hue='Country',\n",
    "    palette='tab10',\n",
    "    ax=ax,\n",
    ")\n",
    "ax.set_title('Original feature space: Access to electricity vs. Internet usage')\n",
    "ax.set_xlabel('Access to electricity (%)')\n",
    "ax.set_ylabel('Internet users (%)')\n",
    "ax.legend(bbox_to_anchor=(1.02, 1), loc='upper left', title='Country')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0251a900",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA-transformed space (PC1 vs PC2)\n",
    "pc_df = pd.DataFrame(projected_optimal[:, :2], columns=['PC1', 'PC2'])\n",
    "pc_df['Country'] = df_clean['Country'].values\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "sns.scatterplot(data=pc_df, x='PC1', y='PC2', hue='Country', palette='tab10', ax=ax)\n",
    "ax.set_title('Principal component space: PC1 vs PC2')\n",
    "ax.set_xlabel('PC1 (highest variance)')\n",
    "ax.set_ylabel('PC2')\n",
    "ax.legend(bbox_to_anchor=(1.02, 1), loc='upper left', title='Country')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57e6db9",
   "metadata": {},
   "source": [
    "## Interpretation\n",
    "- **Variance retention:** The cumulative explained variance plot shows how quickly the first few components capture most of the information, guiding the component selection.\n",
    "- **Structure preservation:** Clusters present in the original scatter plot remain distinguishable after PCA, albeit rotated into the new orthogonal basis.\n",
    "- **Scalability:** Vectorised NumPy operations enable PCA to run efficiently even on larger synthetic datasets, offering confidence that the approach generalises beyond this formative example."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
